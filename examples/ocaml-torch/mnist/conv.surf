let batch_size = 256 in
let epochs = 5000 in
let learning_rate = 0.001 in

(* let device = Device.cuda_if_available () in *)
let vs = Var_store.create "cnn" () in
let conv2d1 {size : Nat} =
  Layer.conv2d_
    {size} {28} {28}
    5 (* ksize *)
    1 (* stride *)
    0 (* padding *)
    1 (* input_dim *)
    32 (* output_dim *)
    vs
in
(* conv2d1 : Tensor %[batch_size, 1, 28, 28] -> Tensor %[batch_size, 32, 24, 24] *)
let conv2d2 {size : Nat} =
  Layer.conv2d_
    {size} {12} {12}
    5 (* ksize *)
    1 (* stride *)
    0 (* padding *)
    32 (* input_dim *)
    64 (* output_dim *)
    vs
in
(* conv2d2 : Tensor %[batch_size, 32, 12, 12] -> Tensor %[batch_size, 64, 8, 8] *)
(* let a = if true then conv2d2 else (fun(b : Int) -> 0) in *)
let linear1 {q : Nat} =
  Layer.linear
    {[q]}
    1024 (* input_dim *)
    1024 (* output_dim *)
    vs
    Layer.Activation.relu
in
(* let a = if true then linear1 else (fun(b : Int) -> 0) in *)
let linear2 {q : Nat} =
  Layer.linear
    {[q]}
    1024 (* input_dim *)
    Mnist_helper.label_count (* output_dim *)
    vs
    Layer.Activation.none
in
let adam = Optimizer.adam vs learning_rate in
let model (size : Nat) (is_training : Bool) (xs : Tensor [size, Mnist_helper.image_dim]) = (* TODO *)
  Tensor.reshape [size, 1, 28, 28] xs
  |> Layer.forward (conv2d1 {size})
        (* Tensor [size, 32, 24, 24] *)
  |> Tensor.max_pool2d
      0 0 (* padding *)
      2 2 (* ksize *)
      2 2 (* stride *)
        (* Tensor [size, 32, 12, 12] *)
  |> Layer.forward (conv2d2 {size})
        (* Tensor [size, 64, 8, 8] *)
  |> Tensor.max_pool2d
      0 0 (* padding *)
      2 2 (* ksize *)
      2 2 (* stride *)
        (* Tensor [size, 64, 4, 4] *)
  |> Tensor.reshape [size, 1024]
  |> Layer.forward (linear1 {size})
  |> Tensor.dropout 0.5 is_training
  |> Layer.forward (linear2 {size})
in
let train_model (size : Nat) = model size true in
let test_model (size : Nat) = model size false in
(* let a = if true then test_model else (fun(b : Int) -> 0) in *)
(range 1 epochs) |> List.iter (fun(batch_idx : Int) ->
  let (batch_images, batch_labels) =
    Dataset_helper.train_batch
      batch_size
      Mnist_helper.train_images
      Mnist_helper.train_labels
      Mnist_helper.test_images
      Mnist_helper.test_labels
      batch_idx
  in
  let loss =
    Tensor.cross_entropy_for_logits
      _ {Mnist_helper.label_count}
      (train_model batch_size batch_images) batch_labels
  in
  Optimizer.backward_step adam loss;
  if mod batch_idx 50 == 0 then
    let test_accuracy =
      Dataset_helper.batch_accuracy
        _ _ {Mnist_helper.label_count}
        batch_size
        test_model
        Mnist_helper.test_images
        Mnist_helper.test_labels
    in
    (* print_int batch_idx *)
    print_float (Tensor.float_value loss);
    print_float test_accuracy
  else
    ()
)
